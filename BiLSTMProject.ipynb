{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b49e123-2420-4b96-b345-94b5ae6a445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"lex_glue\", \"unfair_tos\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "validation_df = pd.DataFrame(dataset['validation'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc2cbd7d-3d29-4c00-8db0-1e71e178af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "Initializing tokenizer...\n",
      "\n",
      "Creating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,052,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">234,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_ids (\u001b[38;5;33mInputLayer\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m3,052,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_11 (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m234,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_12 (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m164,352\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m520\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,459,824</span> (13.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,459,824\u001b[0m (13.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,459,824</span> (13.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,459,824\u001b[0m (13.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing training data...\n",
      "Converting dataset to arrays...\n",
      "Input shape: (5532, 256)\n",
      "Labels shape: (5532, 8)\n",
      "\n",
      "Preparing validation data...\n",
      "Converting dataset to arrays...\n",
      "Input shape: (2275, 256)\n",
      "Labels shape: (2275, 8)\n",
      "\n",
      "Preparing test data...\n",
      "Converting dataset to arrays...\n",
      "Input shape: (1607, 256)\n",
      "Labels shape: (1607, 8)\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 764ms/step - accuracy: 0.1033 - loss: 0.4611 - val_accuracy: 0.0246 - val_loss: 0.0730 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 694ms/step - accuracy: 0.2475 - loss: 0.1003 - val_accuracy: 0.0246 - val_loss: 0.0712 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 685ms/step - accuracy: 0.2305 - loss: 0.0941 - val_accuracy: 0.9284 - val_loss: 0.0705 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 694ms/step - accuracy: 0.2703 - loss: 0.0938 - val_accuracy: 0.9284 - val_loss: 0.0705 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 793ms/step - accuracy: 0.2787 - loss: 0.0887 - val_accuracy: 0.9284 - val_loss: 0.0701 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 647ms/step - accuracy: 0.2999 - loss: 0.0949 - val_accuracy: 0.9284 - val_loss: 0.0701 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 631ms/step - accuracy: 0.3568 - loss: 0.0885 - val_accuracy: 0.9112 - val_loss: 0.0665 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 663ms/step - accuracy: 0.3841 - loss: 0.0813 - val_accuracy: 0.9275 - val_loss: 0.0583 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 649ms/step - accuracy: 0.3025 - loss: 0.0702 - val_accuracy: 0.7692 - val_loss: 0.0569 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 636ms/step - accuracy: 0.2541 - loss: 0.0624 - val_accuracy: 0.9253 - val_loss: 0.0571 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 643ms/step - accuracy: 0.2472 - loss: 0.0606 - val_accuracy: 0.9284 - val_loss: 0.0554 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 648ms/step - accuracy: 0.2902 - loss: 0.0555 - val_accuracy: 0.9257 - val_loss: 0.0546 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 647ms/step - accuracy: 0.2678 - loss: 0.0543 - val_accuracy: 0.1363 - val_loss: 0.0614 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 685ms/step - accuracy: 0.2552 - loss: 0.0529 - val_accuracy: 0.7864 - val_loss: 0.0603 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 651ms/step - accuracy: 0.2796 - loss: 0.0504 - val_accuracy: 0.5262 - val_loss: 0.0556 - learning_rate: 5.0000e-05\n",
      "\n",
      "Evaluating on test set...\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 203ms/step - accuracy: 0.9143 - loss: 0.0531\n",
      "Test loss: 0.0562\n",
      "Test accuracy: 0.9135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\n",
      "Sample prediction:\n",
      "Text: Your data may be shared with third parties without explicit consent.\n",
      "Predicted probabilities: [0.00172717 0.00089705 0.00158081 0.00066358 0.0009528  0.00069263\n",
      " 0.00016231 0.00055171]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"lex_glue\", \"unfair_tos\")\n",
    "\n",
    "# Convert dataset to format suitable for training\n",
    "def prepare_data_for_training(dataset):\n",
    "    print(\"Converting dataset to arrays...\")\n",
    "    \n",
    "    # Get all texts\n",
    "    all_texts = dataset['text']\n",
    "    \n",
    "    # Tokenize all texts\n",
    "    tokenized = tokenizer(all_texts, \n",
    "                         padding='max_length', \n",
    "                         truncation=True, \n",
    "                         max_length=256,\n",
    "                         return_tensors='np')\n",
    "    \n",
    "    # Get input IDs\n",
    "    input_ids = tokenized['input_ids']\n",
    "    \n",
    "    # Handle labels\n",
    "    all_labels = []\n",
    "    for item in dataset:\n",
    "        # Convert each label list to a numpy array\n",
    "        label_array = np.zeros(8)  # Initialize array with zeros\n",
    "        for idx in item['labels']:  # item['labels'] contains indices of positive classes\n",
    "            label_array[idx] = 1\n",
    "        all_labels.append(label_array)\n",
    "    \n",
    "    # Convert to final numpy array\n",
    "    labels = np.array(all_labels)\n",
    "    \n",
    "    print(f\"Input shape: {input_ids.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "    return input_ids, labels\n",
    "\n",
    "# Initialize tokenizer\n",
    "print(\"\\nInitializing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def create_bilstm_model(vocab_size, num_labels=8, embedding_dim=100, max_len=256):\n",
    "    # Input layer\n",
    "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding = layers.Embedding(vocab_size, embedding_dim, input_length=max_len)(input_ids)\n",
    "    \n",
    "    # Add dropout to prevent overfitting\n",
    "    x = layers.Dropout(0.2)(embedding)\n",
    "    \n",
    "    # BiLSTM layers\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    output = layers.Dense(num_labels, activation='sigmoid')(x)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = models.Model(inputs=input_ids, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "print(\"\\nCreating model...\")\n",
    "model = create_bilstm_model(\n",
    "    vocab_size=30522,  # BERT vocab size\n",
    "    num_labels=8,      # Number of label categories\n",
    "    embedding_dim=100, # Size of embedding vectors\n",
    "    max_len=256       # Maximum sequence length\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Prepare data\n",
    "print(\"\\nPreparing training data...\")\n",
    "train_features, train_labels = prepare_data_for_training(train_dataset)\n",
    "print(\"\\nPreparing validation data...\")\n",
    "val_features, val_labels = prepare_data_for_training(validation_dataset)\n",
    "print(\"\\nPreparing test data...\")\n",
    "test_features, test_labels = prepare_data_for_training(test_dataset)\n",
    "\n",
    "# Create callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.weights.h5',  # Fixed filepath\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nStarting training...\")\n",
    "history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_results = model.evaluate(test_features, test_labels)\n",
    "print(f\"Test loss: {test_results[0]:.4f}\")\n",
    "print(f\"Test accuracy: {test_results[1]:.4f}\")\n",
    "\n",
    "# Function to make predictions\n",
    "def make_predictions(text, model, tokenizer):\n",
    "    # Preprocess the text\n",
    "    cleaned_text = text.strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer(\n",
    "        cleaned_text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(tokens['input_ids'])\n",
    "    return prediction[0]\n",
    "\n",
    "# Example prediction\n",
    "sample_text = \"Your data may be shared with third parties without explicit consent.\"\n",
    "predictions = make_predictions(sample_text, model, tokenizer)\n",
    "print(\"\\nSample prediction:\")\n",
    "print(\"Text:\", sample_text)\n",
    "print(\"Predicted probabilities:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "310b1353-bdfc-4442-a3f3-d12f11028580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Results:\n",
      "--------------------------------------------------\n",
      "Jurisdiction              : 0.17%\n",
      "Limitation of Liability   : 0.16%\n",
      "Content Removal           : 0.10%\n",
      "Choice of Law             : 0.09%\n",
      "Contract by Using         : 0.07%\n",
      "Unilateral Change         : 0.07%\n",
      "Arbitration               : 0.06%\n",
      "Unilateral Termination    : 0.02%\n"
     ]
    }
   ],
   "source": [
    "def make_predictions(text, model, tokenizer):\n",
    "    # Mapping of indices to unfair clause categories\n",
    "    category_mapping = {\n",
    "        0: \"Jurisdiction\",\n",
    "        1: \"Choice of Law\",\n",
    "        2: \"Limitation of Liability\",\n",
    "        3: \"Unilateral Change\",\n",
    "        4: \"Content Removal\",\n",
    "        5: \"Contract by Using\",\n",
    "        6: \"Unilateral Termination\",\n",
    "        7: \"Arbitration\"\n",
    "    }\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(tokens['input_ids'], verbose=0)\n",
    "    probabilities = predictions[0]\n",
    "    \n",
    "    # Create readable output\n",
    "    results = []\n",
    "    for idx, prob in enumerate(probabilities):\n",
    "        percentage = prob * 100\n",
    "        category = category_mapping[idx]\n",
    "        results.append({\n",
    "            \"category\": category,\n",
    "            \"probability\": percentage\n",
    "        })\n",
    "    \n",
    "    # Sort by probability in descending order\n",
    "    results.sort(key=lambda x: x[\"probability\"], reverse=True)\n",
    "    \n",
    "    # Print formatted results\n",
    "    print(\"\\nPrediction Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    for result in results:\n",
    "        print(f\"{result['category']:25} : {result['probability']:.2f}%\")\n",
    "    \n",
    "    # Return the structured results\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"Your data may be shared with third parties without explicit consent.\"\n",
    "results = make_predictions(sample_text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf72743-8652-443d-865e-01dff94ee4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset structure\n",
    "print(dataset['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5fc212d-e15c-458f-972c-b85e0a280644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['notice to california subscribers : you may cancel your subscription , without penalty or obligation , at any time prior to midnight of the third business day following the date you subscribed . \\n', 'if you subscribed using your apple id , refunds are handled by apple , not tinder . \\n', 'if you wish to request a refund , please visit https://getsupport.apple.com . \\n', 'if you subscribed using your google play store account or through tinder online : contact customer support \\n', \"key changes in this version : we 've included a legal notice required under california law regarding refunds and updated our legal name to match group , llc \\n\"], 'labels': [[], [], [], [], []]}\n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows of the train dataset\n",
    "print(dataset['train'][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5bb2b-7e37-4dc5-9ced-b0709db1809a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
